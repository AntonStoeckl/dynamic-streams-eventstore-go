## PostgreSQL Memory & GIN Index Optimization
- **Created**: 2025-08-25 12:52
- **Started**: 2025-08-25 12:52
- **Priority**: High
- **Objective**: Fix memory configuration causing I/O bottleneck and optimize GIN index performance

## 🔍 **Updated PostgreSQL Performance Analysis & Optimization Plan**

### Current State Analysis:
- **Table Size**: 11M rows, 4.8GB data + 547MB GIN index  
- **Cache Hit Ratio**: 81.71% (LOW - should be >95%)
- **Statistics**: ✅ **FIXED** - payload now at 300 (was 500)
- **Key Remaining Issue**: Memory configuration causing I/O bottleneck

### Root Causes Identified:

1. **Memory Configuration Mismatch** 🎯 **PRIMARY ISSUE**
   - Container allocated 4GB RAM but PostgreSQL only using 1GB shared_buffers
   - Cache hit ratio at 81% indicates frequent disk I/O  
   - DataFilePrefetch wait events confirm I/O bottleneck

2. **GIN Index Potential Optimization**
   - Default `gin_pending_list_limit = 4096KB` (4MB)
   - Previous attempts failed due to primary key scans instead of GIN usage
   - Now that GIN index is heavily used (8M scans), fastupdate=off might help

### **Refined Optimization Strategy:**

#### **Phase 1: Memory Configuration Fix** 🚀 **HIGH IMPACT**
1. **Increase shared_buffers** to 2GB (50% of 4GB allocation)
2. **Increase effective_cache_size** to 3GB (75% of allocation) 
3. **Set work_mem = 128MB** for better query performance
4. **Set maintenance_work_mem = 512MB** for faster ANALYZE/index operations

#### **Phase 2: GIN Index Optimization** 🔬 **TEST WORTHY**
1. **Disable fastupdate** on GIN indexes (eliminate pending list entirely)
   ```sql
   ALTER INDEX idx_events_payload_gin SET (fastupdate = off);
   ```
2. ~~Partial indexes~~ - Not applicable for library (unknown user predicates)

#### **Phase 3: Autovacuum Tuning** ⏳ **DATA-DRIVEN**
1. ~~Increase analyze threshold~~ - WAIT for statistics=300 results first
2. **Add autovacuum_work_mem = 512MB** for faster index maintenance
3. **Vacuum/analyze separation** - Already implemented in current config ✅

#### **Phase 4: Query Optimization** 🤔 **RESEARCH NEEDED**
1. ~~Connection pooling~~ - Already optimized with master/replica routing ✅
2. **Prepared statements** - Need to research EventStore query patterns first
3. ~~event_type in payload~~ - Already tested, no improvement ✅

### **Next Steps Priority:**
1. **🔥 IMMEDIATE**: Memory configuration (shared_buffers → 2GB)
2. **📊 MONITOR**: Statistics=300 impact on ANALYZE timing
3. **🧪 TEST**: GIN fastupdate=off (now that index is used heavily)
4. **📈 MEASURE**: Cache hit ratio improvement

### **Expected Results:**
- Cache hit ratio: 81% → 95%+
- ANALYZE timing: Already improved with statistics=300
- Query consistency: Reduced I/O-related performance spikes
- Memory efficiency: Better utilization of 4GB allocation

**Focus**: Memory first, then GIN optimization if needed based on data.

## Implementation Progress

### ✅ **User Results with statistics=300** (2025-08-25 13:15)
- Simulation running, still extreme up/down performance
- Auto-analyze takes 30-35 seconds total (including queue time)
- Query plans look good, GIN index being used
- Autotuning constantly reacting to performance swings

### 📊 **Updated Measurements** (2025-08-25 13:20)
- **ANALYZE Performance**: ✅ 4 seconds actual execution (was 15-20s)
- **Cache Hit Ratio**: ❌ 74.08% (worse than 81.71% before)  
- **Memory Config**: 🔴 shared_buffers=1GB, database=7.3GB (BOTTLENECK!)
- **Index Usage**: ✅ GIN index heavily used (84k scans)
- **Query Performance**: 10-12ms mean, high stddev (sawtooth confirmed)

### 🔥 **Root Cause Identified**
The 30-35 second ANALYZE timing is: 4s execution + 26-31s I/O thrashing
Database (7.3GB) is larger than shared_buffers (1GB) causing memory crisis

### **REVISED PRIORITY**: Phase 1 Memory Fix is URGENT

### ✅ **Phase 1 COMPLETED** (2025-08-25 13:30)
- ✅ shared_buffers: 1GB → 2GB (verified: 262144 pages = 2GB)
- ✅ effective_cache_size: 2.4GB → 3GB (verified: 393216 pages = 3GB) 
- ✅ work_mem: 64MB → 128MB (verified: 131072 = 128MB)
- ✅ maintenance_work_mem: 256MB → 512MB (verified: 524288 = 512MB)
- ✅ **STATUS**: All settings applied and verified
- ✅ **Catalog settings**: payload statistics=300, autovacuum thresholds confirmed
- ⏳ **NEXT**: Ready for performance testing and measurement

### 📊 **Post-Memory-Fix Measurements** (2025-08-25 13:38)
- **User Report**: Started very good, then degraded very badly
- **Cache Hit Ratio**: 78.76% (slight improvement from 74%, but not the expected 95%+)
- **ANALYZE Performance**: 3.1 seconds (good - was 4s before memory fix)
- **Query Performance**: Mean 8-9ms (slightly better than 10-12ms)
- **New Issue Discovered**: ANALYZE still causing degradation despite memory fix

### 🔍 **Root Cause Analysis** 
**The memory fix helped but didn't solve the core issue:**
1. **ANALYZE Impact**: Even at 3.1s execution, still causes performance degradation
2. **Frequency**: Running every ~1 minute (1500 row threshold + 50 events/sec)  
3. **Cache Hit Still Low**: 78% indicates the 7.3GB database + working set still too large
4. **Statistics Gathering**: The 300 statistics target still requires significant I/O

### 🎯 **New Insights**
- Memory fix reduced ANALYZE time: 30-35s → 3.1s ✅
- But ANALYZE frequency + I/O impact still causes sawtooth pattern ❌ 
- Need to either: reduce ANALYZE frequency OR reduce statistics target further

### 📊 **Follow-up Measurements** (2025-08-25 13:45)
- **Cache Hit Ratio**: 80.42% (gradually improving from 74% → 78% → 80%)
- **ANALYZE Progress**: Currently running (68,885/90,000 blocks = 77% complete)
- **Query Performance**: 9.8-11.4ms mean, stddev 17-19ms (sawtooth confirmed)
- **Max Query Spikes**: Up to 862ms during ANALYZE operations
- **Database Size**: 7.31GB (still growing, exceeds 2GB shared_buffers)
- **Pattern Confirmed**: "Very good then very bad" matches ANALYZE cycles exactly

### 🎯 **Recommended Solution**: Reduce ANALYZE Impact
**Problem**: Even 3.1s ANALYZE causes severe disruption due to 90k block sampling
**Solution**: Reduce both statistics target AND frequency

### ✅ **Phase 2 COMPLETED** (2025-08-25 13:55)
- ✅ payload statistics: 300 → 150 (verified: attstattarget=150)
- ✅ autovacuum_analyze_threshold: 1500 → 3000 (verified: threshold=3000)
- ✅ **STATUS**: All settings applied and verified
- ⏳ **NEXT**: Ready for performance testing - expecting 50% reduction in ANALYZE impact

### 📊 **Phase 2 Results** (2025-08-25 14:05)
**User Report**: "Less peak performance but flatter"

**Measurements Confirm Trade-off:**
- **Cache Hit Ratio**: 82.73% (continued improvement: 74% → 80% → 82.73%)
- **Query Performance**: Mean 11-14ms (slightly slower than 8-9ms before)
- **Consistency Improved**: Max spikes 429-474ms (vs 862ms before - 45% reduction!)
- **ANALYZE Impact**: Still ~3s execution time (statistics=150 didn't reduce as much as expected)
- **Frequency Reduced**: 32s since last analyze (vs constant 1min cycle before)

### 🎯 **Analysis: Successful Trade-off**
**✅ What Improved (Consistency):**
- Max query spikes: 862ms → 474ms (45% reduction in worst-case)
- ANALYZE frequency: Every 60s → Every 120s+ (50% less frequent)
- Cache hit ratio: 80% → 82.73% (steadily improving)

**⚠️ What Degraded (Peak Performance):**
- Mean query time: 8-9ms → 11-14ms (~40% slower baseline)
- Query planner effectiveness: Reduced statistics = less optimal plans

### 💡 **Root Cause of Trade-off**
**Statistics=150 impact**: Query planner has less information for optimization
**Result**: More consistent but suboptimal execution plans
**Classic trade-off**: Statistics quality vs ANALYZE disruption

### 📊 **Updated Measurements** (2025-08-25 14:10 - simulation off)
- **Cache Hit Ratio**: 83.60% (continuing upward trend)
- **Query Performance**: Mean 10.5-12.8ms, max spikes 429-474ms
- **ANALYZE Status**: 2724 rows since last analyze (approaching 3000 threshold)
- **Last ANALYZE**: 248 seconds ago (~4 minutes - threshold working)
- **User Assessment**: Dislikes both lower max AND lower min performance, too much variability
- **Plan**: User switching to non-auto-tuning simulation + reducing DB size to match laptop resources