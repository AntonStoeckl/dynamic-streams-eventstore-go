## Improve Query Handler Performance (Focus: BooksInCirculation Result Processing)
- **Created**: 2025-08-15 11:35
- **Started**: 2025-08-20 17:40
- **Priority**: High - Performance Optimization
- **Objective**: Optimize query handler performance, particularly BooksInCirculation result processing bottleneck

## Problem Analysis

### Performance Issue Identified
- **Primary bottleneck**: Query handlers, especially `BooksInCirculation`
- **Specific area**: "result_processing" phase taking disproportionately long
- **Impact**: Query operations showing high latency in Grafana duration panels
- **Context**: Issue likely exacerbated under high load from simulation

### Query Handler Architecture
Current query handlers follow pattern:
1. **Query Phase**: Fetch events from EventStore
2. **Result Processing**: Transform events into domain projections ← **BOTTLENECK**
3. **Return**: Deliver processed results

## Investigation Areas

### 1. BooksInCirculation Handler Analysis
**Location**: `/example/features/booksincirculation/`
- **Query method**: Event fetching performance
- **Processing logic**: Event-to-projection transformation efficiency
- **Data structures**: Memory allocation and iteration patterns
- **Algorithmic complexity**: O(n) vs O(n²) processing patterns

### 2. Common Query Handler Patterns
**Related handlers**:
- `BooksLentOut` - Similar processing patterns
- `RegisteredReaders` - Cross-reference for optimization opportunities
- **Pattern analysis**: Shared inefficiencies across handlers

### 3. Event Processing Bottlenecks
**Potential issues**:
- **JSON unmarshalling**: Repeated deserialization overhead
- **Data structure building**: Inefficient map/slice operations  
- **Memory allocations**: Excessive garbage collection pressure
- **Event iteration**: Inefficient loops or filtering

## Optimization Opportunities

### Short-term Improvements
1. **Profiling integration**: Add CPU/memory profiling to identify hotspots
2. **Algorithm optimization**: Review O(n²) loops, optimize data structures
3. **Memory optimization**: Reduce allocations, reuse buffers
4. **Parallel processing**: Concurrent event processing where safe

### Medium-term Enhancements
1. **Caching strategies**: Cache frequently computed projections
2. **Incremental updates**: Avoid full rebuilds on each query
3. **Data structure optimization**: More efficient internal representations
4. **Batch processing**: Group operations to reduce overhead

### Long-term Architecture
1. **CQRS read models**: Pre-computed projections updated on write
2. **Event sourcing optimization**: Snapshot-based rebuilding
3. **Database-level optimization**: Optimized queries, indexes

## Success Criteria

### Performance Targets
- **Latency reduction**: 50%+ improvement in result_processing phase
- **Throughput increase**: Handle higher query volumes without degradation
- **Memory efficiency**: Reduced GC pressure and memory allocations
- **Scalability**: Linear performance scaling with data volume

### Measurement Approach
- **Before/after profiling**: CPU and memory usage comparison
- **Grafana metrics**: Duration panel improvements visible
- **Load testing**: Performance under simulation load
- **Benchmarking**: Quantified improvement metrics

## Implementation Strategy

### Phase 1: Analysis & Profiling
1. **Add profiling hooks** to query handlers
2. **Identify specific bottlenecks** in result_processing
3. **Benchmark current performance** baseline
4. **Document findings** with specific optimization targets

### Phase 2: Targeted Optimizations  
1. **Fix algorithmic inefficiencies** (O(n²) → O(n))
2. **Optimize memory allocations** (reduce GC pressure)
3. **Improve data structures** (maps, slices, iterations)
4. **Test performance impact** of each optimization

### Phase 3: Validation & Monitoring
1. **Load test improvements** under simulation stress
2. **Verify Grafana metrics** show expected improvements  
3. **Regression testing** ensure correctness maintained
4. **Performance monitoring** ongoing optimization opportunities

## Context & Timing

### Why Now?
- **Simulation load testing**: High query volumes exposing bottlenecks
- **New timeout mechanisms**: Proper visibility into performance issues
- **Enhanced Grafana dashboard**: Duration metrics now visible for monitoring
- **Production readiness**: Performance optimization critical for real workloads

### Expected Impact
- **Simulation stability**: Higher throughput without timeouts
- **User experience**: Faster query response times
- **System capacity**: Handle larger data volumes efficiently
- **Resource efficiency**: Lower CPU/memory usage per operation

## Related Work

### Dependencies
- **Profiling tools**: Go pprof, benchmarking infrastructure
- **Testing framework**: Load testing with realistic data volumes
- **Monitoring integration**: Grafana metrics for before/after comparison

### Future Enhancements
- **Query optimization**: Database-level query improvements
- **Caching layer**: Redis or in-memory caching for hot queries  
- **Read replicas**: Query load distribution across database replicas
- **CQRS evolution**: Transition to dedicated read models

---

## 🛠️ **PROGRESS UPDATE: Sequence Number Filtering Foundation**
- **Phase Completed**: 2025-08-20 19:30
- **Context**: Implemented sequence-based filtering infrastructure to enable query performance optimization

### **✅ Phase 1 Complete: Filter Infrastructure**

### **✅ Implementation Complete**
1. **🏗️ Core Architecture**:
   - Added `sequenceNumberHigherThan int64` field to Filter struct
   - Added `SequenceNumberHigherThan() int64` getter method
   - Created `CompletedFilterItemBuilderWithSequenceNumber` interface

2. **🔗 Interface Integration**:
   - Extended all builder interfaces with `WithSequenceNumberHigherThan(sequenceNumber int64)`
   - Enforced mutual exclusivity with time boundaries through type system
   - Maintained fluent builder pattern consistency

3. **⚙️ Type-Safe Implementation**:
   - Complete `WithSequenceNumberHigherThan()` method in filterBuilder
   - Proper state machine transitions between interface types
   - Zero value semantic (0 = not set, sequences start at 1)

### **✅ Comprehensive Testing (47 test cases)**
1. **Valid Combinations (20 cases)**: All filter combinations including sequence-only
2. **Input Sanitization (7 cases)**: Empty values, duplicates, sorting behavior
3. **Mutual Exclusion (4 cases)**: Time vs sequence boundary validation  
4. **Edge Cases (7 cases)**: Zero, negative, max int64 values
5. **Interface Constraints (10 cases)**: Proper `assert.Implements()` validation

### **🎯 Performance Optimization Capability**
- **Sequence-based pagination**: More efficient than time-based for EventStore queries
- **Database optimization**: Sequence numbers are indexed primary keys
- **Query handler enhancement**: Ready for `sequence_number > X` filters in SQL generation

### **📋 Usage Examples**
```go
// Sequence-only filtering for pagination
filter := BuildEventFilter().
    WithSequenceNumberHigherThan(12345).
    Finalize()

// Combined with event types for targeted queries  
filter := BuildEventFilter().
    Matching().
    AnyEventTypeOf("BookCopyLentToReader").
    WithSequenceNumberHigherThan(67890).
    Finalize()
```

### **✅ Phase 1 Achievements**
- **Filter Builder**: Complete sequence number support with sanitization
- **SQL Generation**: PostgreSQL engine generates `sequence_number > ?` clauses
- **Testing**: All integration and unit tests passing
- **Infrastructure**: Ready for query handler integration

### **🔄 Phase 2: Query Handler Integration (Next Session)**
- **BooksInCirculation Optimization**: Replace time-based with sequence-based filtering
- **Performance Measurement**: Benchmark before/after improvements
- **Load Testing**: Validate performance under simulation stress
- **Monitoring**: Grafana metrics showing improved query durations

### **📋 Remaining Work**
1. **Update query handlers** to use `WithSequenceNumberHigherThan()` 
2. **Performance benchmarking** and validation
3. **Documentation** of optimization results
4. **Production testing** under realistic loads

---

## 🚀 **PHASE 2: Generic Snapshotting Implementation**
- **Planned**: 2025-08-20 22:30
- **Context**: Leverage sequence number filtering for event sourcing snapshots to achieve 99.4% performance improvement

### **🎯 Core Solution: JSONB Snapshots with Incremental Updates**
Instead of rebuilding projections from scratch every query, implement generic snapshotting:
1. **Store projection state** as JSONB snapshot + last sequence number
2. **On subsequent queries**: Load snapshot + query only new events since snapshot
3. **Apply incremental updates** to cached projection state  
4. **Update snapshot** with latest sequence number

### **📊 Performance Analysis**

#### **Current Data Scale (BooksInCirculation)**
- **~60,000 books** in circulation (simulation context)
- **BookInfo struct**: BookID, Title, Authors, ISBN, Edition, Publisher, Year, AddedAt, IsCurrentlyLent

#### **JSON Size Estimation**
```json
// Per book: ~220 bytes JSON
{
  "BookID": "book-12345-uuid-format-string-here",       // ~40 bytes
  "Title": "Some Book Title Here",                       // ~50 bytes avg
  "Authors": "Author Name",                              // ~30 bytes avg  
  "ISBN": "978-0123456789",                             // ~15 bytes
  "Edition": "1st Edition",                             // ~15 bytes
  "Publisher": "Publisher Name",                        // ~25 bytes
  "PublicationYear": 2024,                              // ~10 bytes
  "AddedAt": "2025-01-20T14:30:00Z",                   // ~25 bytes
  "IsCurrentlyLent": false                              // ~10 bytes
}
```

#### **Total Snapshot Size**
- **60,000 books × 220 bytes = ~13.2MB**
- **With JSON overhead**: ~15-20MB raw
- **After PostgreSQL JSONB compression**: **~10-12MB stored**
- **PostgreSQL JSONB limit**: 256MB (using <5% of capacity) ✅

#### **Performance Comparison**
| Operation | Current (Full Rebuild) | With Snapshots | Improvement |
|-----------|------------------------|----------------|-------------|
| **Query time** | 180,000ms | **50ms** | **99.97%** |
| **DB queries** | Full table scan | Index lookup + incremental | **99%+** |
| **CPU usage** | Process all events | Parse JSON + incremental | **95%+** |
| **Serialization cost** | N/A | 20-50ms (negligible) | N/A |

### **🏗️ Database Schema Design**

#### **Snapshots Table (Compound Primary Key)**
```sql
CREATE TABLE snapshots (
    projection_type TEXT NOT NULL,           -- "BooksInCirculation"
    filter_hash TEXT NOT NULL,               -- "sha256:abc123def456..."
    sequence_number BIGINT NOT NULL,         -- Last processed sequence number  
    snapshot_data JSONB NOT NULL,            -- The actual projection state
    created_at TIMESTAMPTZ NOT NULL DEFAULT NOW(),
    PRIMARY KEY (projection_type, filter_hash)
);

-- Indexes for cleanup and monitoring
CREATE INDEX idx_snapshots_sequence ON snapshots(sequence_number DESC);
CREATE INDEX idx_snapshots_created_at ON snapshots(created_at DESC);
```

#### **Key Design Decisions**
- **Compound primary key**: `(projection_type, filter_hash)` for efficient lookups
- **No redundant fields**: projection_type + filter_hash uniquely identify snapshot
- **Filter-aware snapshots**: Different filters = different snapshots (critical!)
- **JSONB storage**: Compressed, indexable, 256MB limit well within bounds

### **⚙️ Go Implementation Architecture**

#### **Core Types**
```go
// Snapshot represents a stored projection state
type Snapshot struct {
    ProjectionType string                    // "BooksInCirculation"  
    FilterHash     string                    // "sha256:abc123def456"
    SequenceNumber MaxSequenceNumberUint     // 12345
    Data           json.RawMessage           // Raw JSON bytes
    CreatedAt      time.Time                 
}

// SnapshotStore manages generic snapshot storage/retrieval
type SnapshotStore interface {
    SaveSnapshot(ctx context.Context, snapshot Snapshot) error
    LoadSnapshot(ctx context.Context, projectionType string, filter Filter) (*Snapshot, error)
    DeleteSnapshot(ctx context.Context, projectionType string, filter Filter) error
}

// Filter hashing for snapshot keys
func (f Filter) Hash() string {
    // Hash all filter components: event types, predicates, time bounds, sequence bounds
    hasher := sha256.New()
    hasher.Write([]byte(f.Serialize())) // Deterministic serialization
    return fmt.Sprintf("sha256:%x", hasher.Sum(nil))
}
```

#### **Enhanced EventStore Interface**
```go
type EventStore interface {
    // Existing methods
    Query(ctx context.Context, filter Filter) (StorableEvents, MaxSequenceNumberUint, error)
    Append(ctx context.Context, filter Filter, expectedMaxSeq MaxSequenceNumberUint, events ...StorableEvent) error
    
    // NEW: Snapshot support  
    SaveSnapshot(ctx context.Context, snapshot Snapshot) error
    LoadSnapshot(ctx context.Context, projectionType string, filter Filter) (*Snapshot, error)
    DeleteSnapshot(ctx context.Context, projectionType string, filter Filter) error
}
```

### **🔧 Query Handler Integration Pattern**

#### **BooksInCirculation with Snapshots**
```go
func (h QueryHandler) HandleWithSnapshot(ctx context.Context) (BooksInCirculation, error) {
    baseFilter := BuildEventFilter().
        Matching().
        AnyEventTypeOf(
            core.BookCopyAddedToCirculationEventType,
            core.BookCopyRemovedFromCirculationEventType,
            core.BookCopyLentToReaderEventType,
            core.BookCopyReturnedByReaderEventType,
        ).
        Finalize()
    
    // 1. Try to load existing snapshot
    snapshot, err := h.snapshotStore.LoadSnapshot(ctx, "BooksInCirculation", baseFilter)
    if err == nil && snapshot != nil {
        // 2. Query only incremental events since snapshot
        incrementalFilter := BuildEventFilter().
            WithSequenceNumberHigherThan(int64(snapshot.SequenceNumber)).
            Matching().
            AnyEventTypeOf(
                core.BookCopyAddedToCirculationEventType,
                core.BookCopyRemovedFromCirculationEventType,
                core.BookCopyLentToReaderEventType,
                core.BookCopyReturnedByReaderEventType,
            ).
            Finalize()
            
        newEvents, maxSeq, err := h.eventStore.Query(ctx, incrementalFilter)
        if err != nil {
            // Fallback to full rebuild on incremental query failure
            return h.handleFullRebuild(ctx, baseFilter)
        }
        
        // 3. Pure cache hit - no new events since snapshot
        if len(newEvents) == 0 {
            var cachedResult BooksInCirculation
            json.Unmarshal(snapshot.Data, &cachedResult)
            return cachedResult, nil
        }
        
        // 4. Apply incremental updates to cached projection
        var cachedResult BooksInCirculation
        json.Unmarshal(snapshot.Data, &cachedResult)
        
        newHistory, err := shell.DomainEventsFrom(newEvents)
        if err != nil {
            return BooksInCirculation{}, err
        }
        
        updatedResult := ApplyIncrementalEvents(cachedResult, newHistory)
        
        // 5. Save updated snapshot asynchronously
        go func() {
            updatedSnapshot := Snapshot{
                ProjectionType: "BooksInCirculation",
                FilterHash:     baseFilter.Hash(),
                SequenceNumber: maxSeq,
                Data:          mustMarshal(updatedResult),
                CreatedAt:     time.Now(),
            }
            h.snapshotStore.SaveSnapshot(context.Background(), updatedSnapshot)
        }()
        
        return updatedResult, nil
    }
    
    // 6. Snapshot miss - full rebuild and save new snapshot
    return h.handleFullRebuild(ctx, baseFilter)
}

// ApplyIncrementalEvents applies new events to existing projection state
func ApplyIncrementalEvents(cached BooksInCirculation, newEvents core.DomainEvents) BooksInCirculation {
    // Convert existing books to map for efficient updates
    bookMap := make(map[string]*BookInfo)
    for i := range cached.Books {
        bookMap[cached.Books[i].BookID] = &cached.Books[i]
    }
    
    // Apply incremental changes
    for _, event := range newEvents {
        switch e := event.(type) {
        case core.BookCopyAddedToCirculation:
            if _, exists := bookMap[e.BookID]; !exists {
                bookMap[e.BookID] = &BookInfo{
                    BookID:          e.BookID,
                    Title:           e.Title,
                    Authors:         e.Authors,
                    ISBN:            e.ISBN,
                    Edition:         e.Edition,
                    Publisher:       e.Publisher,
                    PublicationYear: e.PublicationYear,
                    AddedAt:         e.OccurredAt,
                    IsCurrentlyLent: false,
                }
            }
        case core.BookCopyRemovedFromCirculation:
            delete(bookMap, e.BookID)
        case core.BookCopyLentToReader:
            if book := bookMap[e.BookID]; book != nil {
                book.IsCurrentlyLent = true
            }
        case core.BookCopyReturnedByReader:
            if book := bookMap[e.BookID]; book != nil {
                book.IsCurrentlyLent = false
            }
        }
    }
    
    // Convert back to slice and sort
    bookList := make([]BookInfo, 0, len(bookMap))
    for _, book := range bookMap {
        bookList = append(bookList, *book)
    }
    
    slices.SortFunc(bookList, func(a, b BookInfo) int {
        return a.AddedAt.Compare(b.AddedAt)
    })
    
    return BooksInCirculation{
        Books: bookList,
        Count: len(bookList),
    }
}
```

### **🔄 Concurrent Update Handling**

#### **PostgreSQL UPSERT with Sequence Number Protection**
```sql
INSERT INTO snapshots (projection_type, filter_hash, sequence_number, snapshot_data, created_at)
VALUES ($1, $2, $3, $4, NOW())
ON CONFLICT (projection_type, filter_hash) 
DO UPDATE SET 
    sequence_number = GREATEST(snapshots.sequence_number, EXCLUDED.sequence_number),
    snapshot_data = CASE 
        WHEN EXCLUDED.sequence_number >= snapshots.sequence_number 
        THEN EXCLUDED.snapshot_data 
        ELSE snapshots.snapshot_data 
    END,
    created_at = CASE 
        WHEN EXCLUDED.sequence_number >= snapshots.sequence_number 
        THEN EXCLUDED.created_at 
        ELSE snapshots.created_at 
    END;
```

#### **Race Condition Handling**
1. **Concurrent Query A** (seq 1000) and **Query B** (seq 1010) run simultaneously
2. **Query B completes first**, saves snapshot at sequence 1010  
3. **Query A completes later**, attempts to save snapshot at sequence 1000
4. **UPSERT logic**: Compares sequences, keeps snapshot with higher sequence (1010)
5. **Result**: Always converges to most recent snapshot state ✅

### **📋 Implementation Phases**

#### **Phase 2A: Snapshot Infrastructure (2-3 hours)**
1. **Database schema**: Add `snapshots` table with compound primary key
2. **SnapshotStore interface**: Implement in PostgreSQL engine  
3. **Filter hashing**: Create deterministic filter serialization and hashing
4. **Basic CRUD**: SaveSnapshot, LoadSnapshot, DeleteSnapshot methods
5. **UPSERT logic**: Sequence number protection for concurrent updates

#### **Phase 2B: BooksInCirculation Integration (2-3 hours)**  
1. **Update query handler**: Add snapshot-aware handling logic
2. **Incremental updates**: Implement `ApplyIncrementalEvents` function
3. **Fallback logic**: Graceful degradation to full rebuild on snapshot errors
4. **Async snapshot saving**: Non-blocking snapshot updates
5. **Error handling**: Comprehensive error recovery strategies

#### **Phase 2C: Testing & Validation (2-3 hours)**
1. **Unit tests**: Snapshot store CRUD operations
2. **Integration tests**: Query handler with snapshots under various scenarios
3. **Performance testing**: Validate 180s → 50ms improvement target
4. **Concurrent testing**: Verify race condition handling
5. **Load testing**: Performance under simulation stress

#### **Phase 2D: Observability & Monitoring (1 hour)**
1. **Cache metrics**: Hit rates, miss rates, rebuild frequency
2. **Performance metrics**: Snapshot load time, incremental update time
3. **Grafana integration**: Dashboard panels for snapshot performance
4. **Logging**: Snapshot operations and performance events

### **🎯 Success Criteria**

#### **Performance Targets**
- **Latency improvement**: 180s → <100ms (99.4% reduction)
- **Cache hit rate**: >90% for established systems
- **Incremental update time**: <200ms for 1000 new events
- **Memory efficiency**: <50MB snapshot storage per projection

#### **Functional Requirements**
- **Correctness**: Snapshot-based results identical to full rebuild
- **Consistency**: Concurrent updates handled safely
- **Resilience**: Graceful fallback on snapshot corruption/errors  
- **Generic**: Works with any projection type implementing interfaces

### **🚨 Risk Mitigation**

#### **Data Integrity**
- **Fallback strategy**: Always fall back to full rebuild on snapshot errors
- **Validation**: Compare snapshot results with full rebuild in tests
- **Monitoring**: Alert on excessive snapshot misses or errors

#### **Performance Risks**  
- **Large incremental updates**: Limit incremental event count, rebuild if exceeded
- **Snapshot corruption**: Detect and recover from corrupted JSONB data
- **Memory usage**: Monitor snapshot size growth over time

### **✅ Expected Benefits**
- **Massive performance gain**: 180s → 50ms (99.7% improvement)
- **Database load reduction**: 99%+ fewer full table scans
- **Scalability improvement**: Query performance independent of total event count
- **Resource efficiency**: 95%+ reduction in CPU and memory per query
- **Generic solution**: Reusable for all query handlers and command handlers

### **🔄 Future Extensions**
- **Command handler snapshots**: Apply same pattern to aggregate rebuilding
- **Snapshot cleanup policies**: Automatic old snapshot removal
- **Snapshot warming**: Proactive snapshot updates on write operations  
- **Multi-projection snapshots**: Composite snapshots for complex queries

---